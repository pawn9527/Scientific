1. 为什么需要对数值型的特征做归一化?

   - 在特征维数非常多的时候. 可以防治某一特征对最终结果不会造成更大的影响
   - 对于梯度下降求解的模型可以加快算法的收敛速度.
     - 加快算法收敛
     - 提高精度(特别对于需要计算距离的算法)

2. 在对数据进行预处理时, 应该怎样处理类别型特征?

   - 序号编码(Ordinal Encoding)

     通常处理具有大小关系的数据

   - 独热编码(One-hot Encoding)

     不具有大小关系的特征, 把它转换成稀疏矩阵

   - 二进制编码(Binary Encoding)

     1. 选用序号编码给每个类别赋予一个类别的ID
     2. 将类别ID对应的二进制编码作为结果.

3. 在模型估计过程中, 有哪些主要的验证方法, 它们的优缺点是什么?

   - 留出法(hold-out)

     - 定义

       > "留出法" 直接将数据集 D 划分为两个互斥的集合, 其中一个集合作为训练集S, 另一作为测试集 T, 即 D = S & T, S | T = 空集,  在 S 上 训练出模型后, 用 T 来评估其测试误差,  作为对泛化误差的估计.

     - 优点

       > 几乎用所有的数据进行训练, 然后用一个数据进行测试
       >
       > 确定性: 实验没有随机因素, 整个过程是可重复的.

     - 缺点

       > 计算时间很长
       >
       > 分层问题: 若S,T 中样本类别比例差别很大, 则误差估计将由于训练/测试数据分布的差异性而产生偏差

   - 交叉验证法

     - 定义

       > 一个交叉验证将样本数据分为两个互补的子集, 一个子集用于训练分类器或模型, 也被称为训练集(training set); 另一个子集用于验证练出的分类器或模型是否有效, 被称为测试集(testing set). 测试结果作为分类器或模型的性能指标. 而我们的目的是得到高度预测精度和低的预测误差. 为了保证交叉验证结果的稳定性, 对一个样本数据集需要多次不同的划分, 得到不同的互补子集, 进行多次交叉验证. 取多次平均作为验证结果.​	

     - 简单的交叉验证

       - 步骤

         1. 从全部的训练数据 S中随机选择 中随机选择 s的样例作为训练集 train，剩余的 作为测试集 作为测试集 test。
         2. 通过对测试集训练 ，得到假设函数或者模型 。
         3.  在测试集对每一个样本根据假设函数或者模型，得到训练集的类标，求出分类正确率。
         4. 选择具有最大分类率的模型或者假设。

       - 优缺点

         - 优点

           > 由于测试集和训练集是分开的，就避免了过拟合的现象

         - 缺点

           > 此方法浪费了中的数据，即使我们将模型再次带入整个样本集，我们仍然只用了70%的样本建模。如果样本的采集非常的容易以致样本量非常之大，使用交叉验证方法没有什么问题；但如果样本非常稀缺，采集困难，那么我们就需要考虑一种能够充分利用样本的方法.

     - k折交叉验证 k-fold cross validation

       - 步骤

         1. 将全部训练集 S 分为 k 个不相交的子集, 假设S 中的训练样例个数为m, 那么每一个子集有 m/k 个训练样例, 相应的子集称作 ${s_1, s_2, ... , s_k} $
         2. 每次从分好的子集中里面, 拿出一个作为测试集, 其他 k - 1 个作为训练集
         3. 根据训练集训练出模型或者假设函数
         4. 把这个模型放到测试集上, 得到分类率.
         5. 计算 k 次求得的分类率的平均值, 作为该模型或者假设函数的真实分类率.

       - 优缺点

         > 利用了所有样本。但计算比较繁琐，需要训练k次，测试k次.

   - 留一法  leave-one-out cross validation

     - 步骤

       1. 每次只留下一个样本做测试集, 其他样本做训练集
       2. 如果有k个样本, 则需要训练k次, 测试k次.

     - 优缺点:

       > 留一法计算最为繁琐, 但样本利用率最高. 适合于小样本的情况.

   - 自助法
     - 定义

       > 自助法以自助采样为基础(有放回采样). 每次随机从 D 中挑选一个样本, 放入 X 中, 然后将样本放回 D 中, 重复 m 次之后, 得到了包含m个样本的数据集.

     - 优缺点

       - 优点

         自助法在数据集较小, 难以有效划分训练/测试集时很有用, 能从初始数据集中产生多个不同的训练集, 这对集成学习等方法有很大的好处.

       - 缺点

         自助法改变了初始化数据集的分布, 这会引入估计偏差.

4. 在模型评估过程中, 过拟合和欠拟合具体是指什么现象?

   - 过拟合

     > 模型过于简单, 未能充分捕获样本数据的特征. 表现为模型在训练集上的效果不好.

   - 欠拟合

     > 模型过于复杂, 过分捕获样本数据的特征, 从而将样本数据中一些特殊特征当成了共性特征. 表现为模型在训练集上效果非常好, 但是在未知数据上的表现效果不好.

5. 能否说出几种降低过拟合和欠拟合的方法?

   - 过拟合
     - 收集更多的数据
     - 降低模型的复杂度
       - 使用正则化
     - 减少迭代次数
     - 选择简单的模型
   - 欠拟合
     - 增加迭代次数
     - 增加模型复杂度
       - 引入新的特征(多项式扩展)
     - 使用更复杂的模型
       - 由线性模型给为非线性模型

6. 逻辑回归相比线性回归, 有何异同?

   逻辑回归是广义的线性模型.

   |              | 线性回归   | 逻辑回归     |
   | ------------ | ---------- | ------------ |
   | 目的         | 预测       | 分类         |
   | 函数         | 拟合函数   | 预测函数     |
   | 损失函数     | 平方损失   | 似然函数     |
   | 参数计算方式 | 最小二乘法 | 最大似然估计 |

7. 对于二分类问题, 当训练集中正负样本非常不均衡时, 如何处理数据已更好地训练分类模型?

   - 随机过采样

     - 定义

       从少数类样本中随机重复又放回的抽取样本以得到更多样本

     - 缺点

       对少数类样本多次复制, 扩大了数据规模, 增加了模型复杂度, 容易过拟合

     - 解决办法

       SMOTE 算法

       简单来说, 就是对少数类每一个样本x, 从他在少数类样本的k近邻中随机挑选一个样本y, 然后在 x 和 y 的连线上随机选取一点作为新的合成的样本. 这种方法避免了复制少数样本, 增加了样本的多样性, 可以降低过拟合的风险; 但是这种方法会增大类间重叠度, 并且会产生一些不能提供有效信息的样本. 为此出现了Borderline-SMOTE (只给那些处于分类边界上的少数类样本合成新的样本), ADASYN (给不同的少数类样本合成不同个数的新样本)

   - 随机欠采样

     - 定义

       从多数类样本中随机的又放回(或无放回)选取较少的样本

     - 缺点

       丢弃一部分样本, 可能会损失部分有用的信息, 造成模型只学到整体模式的一部分

     - 解决办法

       - Easy Ensemble 算法

         每次从多数类随机抽取一个子集, 和少数类训练一个分类器; 重复若干次, 得到多个分类器, 最终的结果是多个分类器的融合

       - Balance Cascade 算法

         级联结构, 在每一级中从多数类中随机抽取子集, 和少数类训练该级的分类器, 然后从多数样本中剔除掉当前分类器能正确识别的样本, 继续下一级操作, 重复若干次, 最后的结构也是各级分类器的融合

8. 在空间上线性可分的两类点, 分别向 **SVM** 分类的超平面上做投影, 这些点在超平面上的投影仍然是线性可分的吗? 为什么?

   不可能.

   超平面刚好是支持向量的中垂线, 支持向量在中垂线上的投影重合, 肯定不可分.

9. LR和SVM有哪些异同?

   - 相同点

     - 都是线性分类器. 本质上都是求一个最佳分类超平面

     - 都是监督学习算法

     - 都是判别模型. 通过决策函数, 判别输入特征之间的差别来进行分类

       常用的判别模型有: KNN, SVM, LR.

       常用的生成模型有: 朴素贝叶斯, 隐马尔可夫模型.

   - 不同点

     - 损失函数不同

       - LR的损失函数是交叉熵:
         $$
         J(\theta) = -\frac{1}{m}[\sum^m_{i=1}y^{(i)}\log{h_{\theta}(x^{(i)}) + (1 - y^{(i)})log(1 - h_{\theta}(x^{(i)}))}]
         $$
         逻辑回归基于概率理论, 假设样本为正样本的概率可以用 sigmoid 函数来表示, 然后用极大似然估计的方法估计出参数的值.

       - SVM的目标损失函数:
         $$
         L(w, b, \alpha) = \frac{1}{2}||w||^2 - \sum_{i=1}^n \alpha_i(y_i(w^Tx_i + b) - 1)
         $$
         支持向量机基于几何间隔最大化的原理, 认为存在最大几何间隔的分类面为最优分类面

     - 两个模型对数据和参数的敏感程度不同

       SVM 考虑分类边界线附件的样本(决定分类超平面的样本). 在支持向量外添加或减少任何样本点对分类决策面没有任何影响

       LR受所有的数据点的影响. 直接依赖数据分布, 每个样本点都会影响决策面的结果. 如果训练数据不同类别严重不平衡, 则一般需要先对数据做平衡处理, 让不同类别的样本尽量平衡.

     - SVM 基于距离分类, LR 基于概率分类.

       SVM依赖数据表达的距离测度, 所以需要对数据先进行 normalization; LR不受其影响

     - 在解决非线性问题时, 支持向量机采用核函数的机制, 而LR通常不采用核函数的方法.

       SVM算法里，只有少数几个代表支持向量的样本参与分类决策计算，也就是只有少数几个样本需要参与核函数的计算。
       LR算法里，每个样本点都必须参与分类决策的计算过程，也就是说，假设我们在LR里也运用核函数的原理，那么每个样本点都必须参与核计算，这带来的计算复杂度是相当高的。尤其是数据量很大时，我们无法承受。所以，在具体应用时，LR很少运用核函数机制。

     - 在小规模数据集上, Linear SVM要略好于LR, 但差别也不是特别大, 而且Linear SVM 的计算复杂度数据量限制, 对海量数据LR使用更加广泛.

     - SVM的损失函数就自带正则($\frac{1}{2}||w||$) , 而LR必须另外在损失函数之外添加正则项.

10. 举例解释朴素贝叶斯模型的原理?

    朴素贝叶斯是基于贝叶斯定理与特征条件独立假设的分类方法.

​       举例: 

​		以某小区为样本, 标记$Y$感染了新型冠状病毒($Y=-1$),  未感染 ($Y=1$). 

​		有两项特征: $x_1$去过或者经过武汉(0, 1),   $x_2$出行十分戴口罩 (0, 1)

​                判别: 没有去过武汉且出行戴口罩是否感染 $y$             

​		计算 $y_1 = P(Y=1)P(x_1=0|Y=1)P(x_2=0|Y=1)$			  

​			和 $y_2 = P(Y=-1)P(x_1=0|Y=-1)P(x_2=0|Y=-1)$

​		结论:

​			如果 $y_1 > y_2$ 则 $y = 1$ 否则 $y=-1$

